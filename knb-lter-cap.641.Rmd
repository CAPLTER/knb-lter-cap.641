---
title: "knb-lter-cap.641"
output: html_document
editor_options: 
  chunk_output_type: console
---

# README

content moved to project README.md

# bird observations

## observations: query

note that this select statement does not include wind speed, wind dir, air
temp, or cloud cover as those variables have not been collected for a long time
and are not relevant to the more recent SRBP observations

```{r srbp-bird-observations}

bird_observations <- DBI::dbGetQuery(pg, "
SELECT
  surveys.id AS survey_id,
  SPLIT_PART(sites.site_code, '_', 1) AS reach,
  sites.site_code,
  surveys.survey_date,
  surveys.time_start,
  surveys.time_end,
  observers.observer,
  bird_taxa.code,
  bird_taxa.common_name,
  bird_observations.distance,
  bird_observations.bird_count,
  bird_observations.notes AS observation_notes,
  bird_observations.seen,
  bird_observations.heard,
  bird_observations.direction,
  bird_observations.qccomment
FROM core_birds.surveys
JOIN core_birds.sites ON (surveys.site_id = sites.id)
JOIN core_birds.bird_observations ON (bird_observations.survey_id = surveys.id)
JOIN core_birds.bird_taxa ON (bird_observations.bird_taxon_id = bird_taxa.id)
JOIN core_birds.observers ON (observers.id = surveys.observer_id)
WHERE
surveys.id IN (
  SELECT
    srbp_surveys.id
  FROM(
    SELECT
      surveys.id,
      sites.site_code,
      SPLIT_PART(sites.site_code, '_', 1) AS reach,
      surveys.survey_date,
      observers.observer
    FROM core_birds.surveys
    JOIN core_birds.sites ON (surveys.site_id = sites.id)
    JOIN core_birds.observers ON (observers.id = surveys.observer_id)
    JOIN (
      SELECT
        SPLIT_PART(sites.site_code, '_', 1) AS reach,
        surveys.survey_date,
        observers.observer
      FROM core_birds.surveys
      JOIN core_birds.sites ON (surveys.site_id = sites.id)
      JOIN core_birds.observers ON (observers.id = surveys.observer_id)
      WHERE
        sites.location_type LIKE 'SRBP'
      GROUP BY
        reach,
        surveys.survey_date,
        observers.observer
      HAVING
      count(*) > 1
      ) AS subquery ON (
        reach = subquery.reach AND
        surveys.survey_date = subquery.survey_date AND
        observers.observer  = subquery.observer
    )
  ) AS srbp_surveys
)
ORDER BY surveys.survey_date
;
")

bird_observations[bird_observations == ""] <- NA # lots of missing values, convert to NA

```

## observations: observers

```{r bird-observations-observers, eval=TRUE}

bird_observations <- bird_observations |>
  tidyr::separate(observer, c("name1", "name2", "name3"), " ", remove = T) |>
  dplyr::mutate(
    namePart1 = tools::toTitleCase(stringr::str_extract(name1, "\\b\\w{2}")),
    namePart2 = tools::toTitleCase(stringr::str_extract(name2, "\\b\\w{2}")),
    namePart3 = tools::toTitleCase(stringr::str_extract(name3, "\\b\\w{2}"))
    ) |>
  dplyr::mutate(
    observer = dplyr::case_when(
      is.na(namePart3) ~ paste0(namePart1, namePart2),
      !is.na(namePart3) ~ paste0(namePart1, namePart2, namePart3)
    )
    ) |>
  dplyr::select(
    -name1,
    -name2,
    -name3,
    -contains("namePart")
  )

```

## observations: conversions

```{r bird-observations-conversions, eval=TRUE}

bird_observations <- bird_observations |>
  dplyr::mutate(
    survey_id   = as.character(survey_id),
    reach       = as.factor(reach),
    survey_date = as.Date(survey_date),
    distance    = as.factor(distance),
    seen        = as.factor(seen),
    heard       = as.factor(heard),
    direction   = as.factor(direction)
    ) |>
  dplyr::select(
    survey_id:time_end,
    observer,
    code:last_col()
  )

```

## observations: filter

The `core-birds-SQL` chunk will query all observation data in the database but
these are not necessarily QC'd; rather than editing the SQL query, we can cull
the full sample set to only those data that are QC'd in this workflow.
Addressed at this step as survey_date has been explicitly declared a date type
in the above chunk.

```{r bird-observations-filter, eval=TRUE}

bird_observations <- bird_observations |>
  dplyr::filter(survey_date <= "2019-09-01")

```

## observations: data table

```{r bird-observations-DT, eval=TRUE}

try({
  capeml::write_attributes(bird_observations, overwrite = FALSE)
  capeml::write_factors(bird_observations, overwrite = FALSE)
})

```

# bird surveys

## surveys: query

```{r bird-surveys-query, eval=TRUE, echo=TRUE, message=TRUE}

bird_surveys <- DBI::dbGetQuery(pg, "
SELECT
  surveys.id AS survey_id,
  SPLIT_PART(sites.site_code, '_', 1) AS reach,
  sites.site_code,
  surveys.survey_date,
  surveys.time_start,
  surveys.time_end,
  observers.observer,
  -- surveys.notes AS survey_notes,
  -- surveys.human_activity_notes,
  surveys.wind,
  surveys.precipitation,
  surveys.disturbances,
  surveys.sight_obstruct,
  surveys.noise_level,
  surveys.site_condition,
  surveys.non_bird_species,
  additional_birds.additional_bird_observations
FROM core_birds.surveys
JOIN core_birds.sites ON (surveys.site_id = sites.id)
JOIN core_birds.observers ON (observers.id = surveys.observer_id)
LEFT JOIN (
  SELECT
    survey_id,
    STRING_AGG(bird_taxa.code, '; ') AS additional_bird_observations
  FROM core_birds.additional_bird_observations
  JOIN core_birds.bird_taxa ON (bird_taxa.id = additional_bird_observations.bird_taxon_id)
  GROUP BY survey_id
) AS additional_birds ON (additional_birds.survey_id = surveys.id)
WHERE
surveys.id IN (
  SELECT
    srbp_surveys.id
  FROM(
    SELECT
      surveys.id,
      sites.site_code,
      SPLIT_PART(sites.site_code, '_', 1) AS reach,
      surveys.survey_date,
      observers.observer
    FROM core_birds.surveys
    JOIN core_birds.sites ON (surveys.site_id = sites.id)
    JOIN core_birds.observers ON (observers.id = surveys.observer_id)
    JOIN (
      SELECT
        SPLIT_PART(sites.site_code, '_', 1) AS reach,
        surveys.survey_date,
        observers.observer
      FROM core_birds.surveys
      JOIN core_birds.sites ON (surveys.site_id = sites.id)
      JOIN core_birds.observers ON (observers.id = surveys.observer_id)
      WHERE
        sites.location_type LIKE 'SRBP'
      GROUP BY
        reach,
        surveys.survey_date,
        observers.observer
      HAVING
      count(*) > 1
      ) AS subquery ON (
        reach = subquery.reach AND
        surveys.survey_date = subquery.survey_date AND
        observers.observer  = subquery.observer
    )
  ) AS srbp_surveys
)
ORDER BY surveys.survey_date
;
")

# convert any missing to NA
bird_surveys[bird_surveys == ""] <- NA
  
```

## surveys: observers

```{r bird-surveys-observers, eval=TRUE}

bird_surveys <- bird_surveys |>
  tidyr::separate(observer, c("name1", "name2", "name3"), " ", remove = T) |>
  dplyr::mutate(
    namePart1 = tools::toTitleCase(stringr::str_extract(name1, "\\b\\w{2}")),
    namePart2 = tools::toTitleCase(stringr::str_extract(name2, "\\b\\w{2}")),
    namePart3 = tools::toTitleCase(stringr::str_extract(name3, "\\b\\w{2}"))
    ) |>
  dplyr::mutate(
    observer = dplyr::case_when(
      is.na(namePart3) ~ paste0(namePart1, namePart2),
      !is.na(namePart3) ~ paste0(namePart1, namePart2, namePart3)
    )
    ) |>
  dplyr::select(
    -name1,
    -name2,
    -name3,
    -contains("namePart")
  )

```

## surveys: conversions

```{r bird-surveys-conversions, eval=TRUE}

bird_surveys <- bird_surveys |>
  dplyr::mutate(
    survey_id     = as.character(survey_id),
    reach         = as.factor(reach),
    survey_date   = as.Date(survey_date),
    wind          = as.factor(wind),
    precipitation = as.factor(precipitation),
    disturbances  = as.factor(disturbances),
    noise_level   = as.factor(noise_level)
    ) |>
  dplyr::select(
    survey_id:time_end,
    observer,
    wind:last_col()
  )

```

## surveys: filter

The `core-birds-SQL` chunk will query all observation data in the database but
these are not necessarily QC'd; rather than editing the SQL query, we can cull
the full sample set to only those data that are QC'd in this workflow.
Addressed at this step as survey_date has been explicitly declared a date type
in the above chunk.

```{r bird-surveys-filter, eval=TRUE}

bird_surveys <- bird_surveys |>
  dplyr::filter(survey_date <= "2019-09-01")

```

## surveys: data table

```{r bird-surveys, eval=TRUE}

dplyr::glimpse(bird_surveys)

try({
  capeml::write_attributes(bird_surveys, overwrite = FALSE)
  capeml::write_factors(bird_surveys, overwrite = FALSE)
})

```

There is not enough detail concerning the SRBP sites in the lter34.sites table
to warrant pulling that information. However, SRBP reach characteristics are
relevant so we will publish those details

# reach characteristics

## reach characteristics: query

```{r reach-characteristics-query}

reach_characteristics <- DBI::dbGetQuery(pg, "
  SELECT
    s.site_code
  FROM core_birds.sites s
  WHERE
    s.location_type LIKE 'SRBP'
  ORDER BY
    site_code
  ;"
  ) |>
  dplyr::mutate(reach = stringr::str_extract(site_code, "^[^_]+")) |>
  dplyr::select(site_code, reach)

herps_reach_chars <- dbGetQuery(pg, "
  SELECT
    reach,
    urbanized,
    restored,
    water
  FROM herpetofauna.river_reaches
  ;"
)

reach_characteristics <- dplyr::inner_join(
  reach_characteristics,
  herps_reach_chars,
  by = c("reach")
  ) |>
  dplyr::mutate(
    reach     = as.factor(reach),
    urbanized = as.factor(urbanized),
    restored  = as.factor(restored),
    water     = as.factor(water)
  )

```

## reach characteristics: data table

```{r reach-characteristics-table}

# reach_characteristics_desc <- "Salt River reach location of each sampling site and general characteristics of that portion of the river where sampling is conducted"

try({
  capeml::write_attributes(reach_characteristics, overwrite = FALSE)
  capeml::write_factors(reach_characteristics, overwrite = FALSE)
})

```

# survey locations 

Get the bird survey locations. Here we are extracting these data from the
database as opposed to using an existing shapefile as I am presenting only the
most up-to-date location information (as opposed to the locations and their
changes through time). Double-check the query, it worked with the small number
of SRBP sites with updated locations at the time these were pulled but not sure
its accuracy when the data are more complicated (e.g., a given location having
moved multiple times) and note that I am using only year to reflect the most
recent location, if a site moved twice in a year, month would have to be
considered as well. Also note that I had to include blh.end_date_year in the
query to be able to include it in the HAVING clause.

As with the herp plots, I am struggling as to whether it is appropriate to even
make the exact points public information from a disturbance standpoint, but
also a too-much-knowledge about the organisms standpoint. Finally, now that we
are tracking the movement of plots through time, that would have to be
encapsulated in this publication so as not to confuse a researcher thinking
that all data necessarily come from the same location. Taken together, I think
the better approach is to make a polygon that encapsulates all points (curent
and historical) at each reach. This provides a user with a reasonable
understanding of the location, but obscures (slightly, anyway) the exact
position details.

see core birds for the query if you want only the current sites

## survey locations: query

```{r survey-locations-query}

survey_locations <- DBI::dbGetQuery(pg, "
  SELECT
    sites.site_code,
    location_histories.lat,
    location_histories.long
  FROM core_birds.location_histories
  JOIN core_birds.sites ON (sites.id = location_histories.site_id)
  WHERE
    sites.location_type LIKE 'SRBP'
  ORDER BY site_code
  ;
  ") |>
  dplyr::mutate(reach = stringr::str_extract(site_code, "^[^_]+")) |>
  dplyr::select(
    reach,
    lat,
    long
  )

survey_locations <- sf::st_as_sf(
    x      = survey_locations,
    coords = c("long", "lat"),
    crs    = 4326
    ) |>
  dplyr::group_by(reach) |>
  dplyr::summarise() |>
  sf::st_convex_hull()

mapview::mapview(survey_locations)

```

## survey locations: vector

```{r survey-locations-vector}

try(capeml::write_attributes(survey_locations, overwrite = FALSE))

survey_locations_desc <- "bird survey locations at select locations along the Salt River in the greater Phoenix metropolitan area; polygons reflect the combined area of all bird survey locations (current and historic) at each river reach"

survey_locations_SV <- capemlGIS::create_vector(
  vector_name = survey_locations,
  description = survey_locations_desc
)

```

# people

```{r people}

# creators

heather <- gioseml::create_role(
  firstName = "heather",
  lastName  = "bateman",
  roleType  = "creator"
)

paige <- gioseml::create_role(
  firstName = "paige",
  lastName  = "warren",
  roleType  = "creator"
)

creators <- list(heather, paige)

# metadata providers

stevan <- gioseml::create_role(
  firstName = "stevan",
  lastName  = "earl",
  roleType  = "meta"
)

sally <- gioseml::create_role(
  firstName = "sally",
  lastName  = "wittlinger",
  roleType  = "meta"
)

metadataProvider <- list(sally, stevan)

```

# coverages

```{r coverages}

begindate <- as.character(min(bird_surveys$survey_date))
enddate   <- as.character(max(bird_surveys$survey_date))
geo_desc  <- yaml::yaml.load_file("config.yaml")$geographic_description

coverage <- EML::set_coverage(
  begin                 = begindate,
  end                   = enddate,
  geographicDescription = geo_desc,
  west                  = sf::st_bbox(survey_locations)[["xmin"]],
  east                  = sf::st_bbox(survey_locations)[["xmax"]],
  north                 = sf::st_bbox(survey_locations)[["ymax"]],
  south                 = sf::st_bbox(survey_locations)[["ymin"]]
)

```

## taxonomic coverage

```{r taxonomyCleanr, eval=FALSE}

my_path <- getwd() # taxonomyCleanr requires a path (to build the taxa_map)

# create or update map. A taxa_map.csv is the heart of taxonomyCleanr. This
# function will build the taxa_map.csv and put it in the path identified with
# my_path.
taxonomyCleanr::create_taxa_map(
  path = my_path,
  x    = bird_observations,
  col  = "common_name"
)

taxonomyCleanr::resolve_comm_taxa(
  path         = my_path,
  data.sources = 3
) # 3 ~ ITIS

# build the EML taxonomomic coverage
taxaCoverage <- taxonomyCleanr::make_taxonomicCoverage(path = my_path)

# add taxonomic to other coverages
coverage$taxonomicCoverage <- taxaCoverage

```

# dataset

Optionally, provide: scope, abstract, methods, keywords, publication date.
Projects scopes include lter (default), urex, ltreb, and som.

```{r construct-dataset}

dataset <- capeml::create_dataset()
```

# eml

```{r construct-eml, eval=TRUE}

eml <- capeml::create_eml()
```

```{r validate-eml, eval=TRUE}

EML::eml_validate(eml)
```

```{r eml-to-file, eval=TRUE}

# write the eml to file
capeml::write_cap_eml()
```

# file placement

```{r package-details, eval=TRUE}

# retrieve package details from config.yaml
if (!file.exists("config.yaml")) {
  stop("config.yaml not found")
}
package_configs <- capeml::read_package_configuration()
identifier      <- package_configs$identifier
scope           <- package_configs$scope

version <- capeml::get_next_version(
  provided_scope      = scope,
  provided_identifier = identifier
)

```

```{r preview_data_file_to_upload}

# preview data set files that will be uploaded to S3
(data_files_to_upload <- list.files(pattern = "^641_"))
```

Move data and final xml files to respective ASU locations.

```{r S3_helper_functions}

# functions and setting for uploading to S3
library(aws.s3)
source("~/Documents/localSettings/aws.s3")
```

```{r upload_data_S3}

# upload files to S3
lapply(data_files_to_upload, gioseml::data_to_amz)
```

# EDI

## EDI: login

```{r edi-login, eval=TRUE, echo=TRUE, message=TRUE}

EDIutils::login(
  userId   = keyring::key_get("edi_user", keyring = "edi"),
  userPass = keyring::key_get("edi_pass", keyring = "edi")
)

```

## EDI: evaluate

```{r edi-evaluate, eval=TRUE, echo=TRUE, message=TRUE}

evaluation <- EDIutils::evaluate_data_package(
  eml         = paste(scope, identifier, version, "xml", sep = "."),
  useChecksum = FALSE,
  env         = "staging"
)

Sys.sleep(8)

eval_status <- EDIutils::check_status_evaluate(
  transaction = evaluation,
  env         = "staging"
)

if (eval_status) {

  # evaluation summary

  EDIutils::read_evaluate_report_summary(
    transaction = evaluation,
    env         = "staging"
  )

}

# evaluation detailed

# EDIutils::read_evaluate_report(
#   transaction = evaluation,
#   env         = "staging"
# )

```

## EDI: update

```{r edi-update, eval=TRUE, echo=TRUE, message=TRUE}

EDIutils::update_data_package(
  eml         = paste(scope, identifier, version, "xml", sep = "."),
  useChecksum = TRUE,
  env         = "production"
)

```

## EDI: logout

```{r edi-logout, eval=TRUE, echo=TRUE, message=TRUE}

EDIutils::logout()

```


# post processing

remove data files (if desired)

```{r delete-data-files}

file.remove(data_files_to_upload)
```

XML/EML file to Amazon and cap-metadata

```{r delete-data-files}

gioseml::eml_to_amz(list.files(pattern = "knb.+xml"))

file.copy(list.files(pattern = "knb.+xml"), "/home/srearl/localRepos/cap-metadata/cap-data-eml/")
file.remove(list.files(pattern = "knb.+xml"))
```
